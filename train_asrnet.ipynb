{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.99\n",
    "\n",
    "POOL_SIZE = BATCH_SIZE * 5\n",
    "RESTORE_CHECKPOINT = False\n",
    "SAVE_GAN_WEIGHTS = True\n",
    "SAVE_EMA_MODEL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 22:32:49.552663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/khoa/anaconda3/envs/nine/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-05-21 22:32:49.552716: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-21 22:32:49.552736: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (zen): /proc/driver/nvidia/version does not exist\n",
      "2022-05-21 22:32:49.552975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from glob import glob\n",
    "from utils.train import no_gan_inner_step, AverageModelWeights\n",
    "\n",
    "from keras.optimizers.optimizer_experimental.adamw import AdamW\n",
    "\n",
    "from utils.data import load_function, feed_data, PoolData\n",
    "from utils.data import feed_props_1, feed_props_2\n",
    "from utils.data import usm_sharpener\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(learning_rate=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.models import RRDBNet\n",
    "\n",
    "no_gan_model = RRDBNet()\n",
    "no_gan_model.build((None, 256, 256, 3))\n",
    "\n",
    "ema_no_gan_model = RRDBNet()\n",
    "ema_no_gan_model.build((None, 256, 256, 3))\n",
    "\n",
    "ema_api = AverageModelWeights(ema_no_gan_model, no_gan_model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath(\"./DIV2K_train_HR/*.png\")\n",
    "train_images_paths = sorted(glob(data_path))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_paths))\n",
    "train_dataset = train_dataset.shuffle(len(train_images_paths))\n",
    "train_dataset = train_dataset.map(\n",
    "    load_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_generator = train_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train_data = PoolData(POOL_SIZE, BATCH_SIZE)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "ema_checkpoint_dir = './ema_training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(model=no_gan_model, optimizer=optimizer)\n",
    "\n",
    "ema_checkpoint_prefix = os.path.join(ema_checkpoint_dir, \"ema_ckpt\")\n",
    "ema_checkpoint = tf.train.Checkpoint(model=ema_no_gan_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESTORE_CHECKPOINT:\n",
    "    print(\"loading training checkpoints: \")\n",
    "    print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    print(\"loading EMA training checkpoints: \")\n",
    "    print(tf.train.latest_checkpoint(ema_checkpoint_dir))\n",
    "    ema_checkpoint.restore(tf.train.latest_checkpoint(ema_checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(gt_images, lq_images):\n",
    "    return no_gan_inner_step(gt_images, lq_images, no_gan_model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = EPOCHS\n",
    "start_epoch = 0\n",
    "\n",
    "train_steps = int(len(train_images_paths) // BATCH_SIZE)\n",
    "\n",
    "pool_train_data = PoolData(POOL_SIZE, BATCH_SIZE)\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean()\n",
    "loss_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    print(\"Start Training\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        train_loss_metric.reset_states()\n",
    "        epoch_time = time.time()\n",
    "        batch_time = time.time()\n",
    "        step = 0\n",
    "\n",
    "        epoch_count = f\"0{epoch + 1}/{epochs}\" if epoch < 9 else f\"{epoch + 1}/{epochs}\"\n",
    "\n",
    "        for img, first_kernel, second_kernel, sinc_kernel in train_generator:\n",
    "            gt_img, lq_img = feed_data(img, first_kernel, second_kernel, sinc_kernel, [\n",
    "                                       feed_props_1, feed_props_2])\n",
    "            gt_img, lq_img = pool_train_data.get_pool_data(gt_img, lq_img)\n",
    "            gt_img = usm_sharpener.sharp(gt_img)\n",
    "            loss = train_step(gt_img, lq_img)\n",
    "\n",
    "            print('\\r', 'Epoch', epoch_count, '| Step', f\"{step}/{train_steps}\",\n",
    "                  '| Loss:', f\"{loss:.5f}\", \"| Step Time:\", f\"{time.time() - batch_time:.2f}\", end='')\n",
    "\n",
    "            train_loss_metric.update_state(loss)\n",
    "            loss = train_loss_metric.result().numpy()\n",
    "            step += 1\n",
    "\n",
    "            loss_results.append(loss)\n",
    "\n",
    "            batch_time = time.time()\n",
    "\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        ema_api.compute_ema_weights(no_gan_model)\n",
    "        ema_checkpoint.save(file_prefix=ema_checkpoint_prefix)\n",
    "\n",
    "        print('\\r', 'Epoch', epoch_count, '| Step', f\"{step}/{train_steps}\",\n",
    "              '| Loss:', f\"{loss:.5f}\", \"| Epoch Time:\", f\"{time.time() - epoch_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs)\n",
    "\n",
    "if SAVE_NORMAL_MODEL:\n",
    "    no_gan_model.save_weights('./checkpoint_weights/last_weights')\n",
    "\n",
    "if SAVE_EMA_MODEL:\n",
    "    ema_no_gan_model.save(\"./no_gan_ema_model\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "233ec9f9255cb3627cc87307a0c74153c014738cac9af1148843b79aa68706f8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nine')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
